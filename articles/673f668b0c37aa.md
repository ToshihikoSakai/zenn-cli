---
title: "OpenLLMを試す"
emoji: "✨"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [python, AI, LLM]
published: true
---

# OpenLLMとは

>An open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.

大規模言語モデル（LLM）を運用するためのオープンプラットフォームです。あらゆるLLMを簡単に微調整、提供、デプロイ、監視できます。

https://github.com/bentoml/OpenLLM

# OpenLLMを動かす

Getting Startedに従って動かしてみます。
Python環境は`Python 3.8`より新しいバージョンが必要です。
パッケージのコンフリクトを防ぐため、仮想環境の利用が推奨されています。

## pipでインストール

```
pip install openllm
```

```
openllm -h
```

## LLMサーバの立ち上げ

以下はメタの`OPT`モデルで起動する場合です。

```
openllm start opt
```

コマンドを実行すると、`OPT`モデルの一式がダウンロードされます。
モデルのダウンロード後は、`http://localhost:3000`にてアクセス可能なBentoServerが起動します。

## WebUI上で確認

WebUI上では、簡易的にLLMを試すことができる。今回は、`OPT`モデルでの応答を試します。
`http://localhost:3000`へアクセスします。

Service APIsの/v1/generateの`Try it out`を押してみます。

![](/images/ServiceAPIs_tryitout.png)

Executeボタンが出てくるので、押すと、Request bodyに応じた問い合わせ結果が出力されます。

Request bodyのprompt部分に質問を入れてみます。

```
"prompt": "Where is the capital of Japan?"
```

curlでの問い合わせは以下です。
```
curl -X 'POST' \
  'http://localhost:3000/v1/generate' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "prompt": "Where is the capital of Japan?",
  "llm_config": {
    "format_outputs": false,
    "max_new_tokens": 1024,
    "min_length": 0,
    "early_stopping": false,
    "num_beams": 1,
    "num_beam_groups": 1,
    "use_cache": true,
    "temperature": 0.75,
    "top_k": 15,
    "top_p": 1,
    "typical_p": 1,
    "epsilon_cutoff": 0,
    "eta_cutoff": 0,
    "diversity_penalty": 0,
    "repetition_penalty": 1,
    "encoder_repetition_penalty": 1,
    "length_penalty": 1,
    "no_repeat_ngram_size": 0,
    "renormalize_logits": false,
    "remove_invalid_values": false,
    "num_return_sequences": 1,
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "encoder_no_repeat_ngram_size": 0
  }
}'
```

Response bodyの結果は以下となりました。きちんと回答が返ってきています。

```
{
  "responses": [
    "Where is the capital of Japan?\nTokyo."
  ],
  "configuration": {
    "format_outputs": false,
    "generation_config": {
      "max_new_tokens": 1024,
      "min_length": 0,
      "early_stopping": false,
      "num_beams": 1,
      "num_beam_groups": 1,
      "use_cache": true,
      "temperature": 0.75,
      "top_k": 15,
      "top_p": 1,
      "typical_p": 1,
      "epsilon_cutoff": 0,
      "eta_cutoff": 0,
      "diversity_penalty": 0,
      "repetition_penalty": 1,
      "encoder_repetition_penalty": 1,
      "length_penalty": 1,
      "no_repeat_ngram_size": 0,
      "renormalize_logits": false,
      "remove_invalid_values": false,
      "num_return_sequences": 1,
      "output_attentions": false,
      "output_hidden_states": false,
      "output_scores": false,
      "encoder_no_repeat_ngram_size": 0
    }
  }
}
```

## ターミナルで確認

ターミナル上でもLLMの回答を確認することができます。

`opt`モデルでサーバを起動します。

```
openllm start opt
```

サーバを起動した状態で以下を実行します。

```
python
>>> import openllm
>>> client = openllm.client.HTTPClient('http://localhost:3000')
>>> client.query('Explain to me the difference between "further" and "farther"')
'Explain to me the difference between "further" and "farther"\nFarther means farther from the source. It\'s like a fart but not as bad.'

# DeepLによる日本語訳
"further"と "farfer"の違いを説明してください。fartherは発生源から遠いという意味。おならみたいだけど、それほどひどくない。
```









